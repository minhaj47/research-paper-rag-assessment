# ü§ñ Ollama Setup Guide

## Current Status

Your RAG system is **running successfully**, but needs Ollama for LLM-powered answers.

**Error you saw**:

```
"Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible."
```

## Quick Fix: Install & Start Ollama

### Option 1: Install Ollama (Recommended)

```bash
# 1. Install Ollama (macOS/Linux)
curl https://ollama.ai/install.sh | sh

# 2. Pull the llama3 model
ollama pull llama3

# 3. Start Ollama (runs in background)
ollama serve
```

### Option 2: Use Ollama with Docker

```bash
# Pull Ollama Docker image
docker pull ollama/ollama

# Run Ollama container
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Pull llama3 model
docker exec -it ollama ollama pull llama3
```

### Verify Ollama is Running

```bash
# Check if Ollama is accessible
curl http://localhost:11434/api/version

# Should return something like:
# {"version":"0.1.x"}
```

## Test Your System Again

Once Ollama is running, try your query again:

```bash
curl -X POST "http://localhost:8000/api/query?query=What%20is%20Deep%20learning&top_k=5"
```

## What's Working Without Ollama

Even without Ollama, your system has:

- ‚úÖ Document upload and processing
- ‚úÖ Vector embeddings generation
- ‚úÖ Semantic search (Qdrant)
- ‚úÖ Metadata storage (PostgreSQL)
- ‚úÖ Context retrieval

**What needs Ollama:**

- ‚ùå LLM answer generation (the actual text response)

## System Architecture

```
Your Query ‚Üí RAG Pipeline ‚Üí Qdrant (Vector Search) ‚Üí ‚úÖ Works!
                          ‚Üì
                    Context Retrieved ‚Üí ‚úÖ Works!
                          ‚Üì
                    Ollama (LLM) ‚Üí ‚ùå Needs to be running
                          ‚Üì
                    Generated Answer
```

## Alternative: Use OpenAI or Other LLM

If you prefer not to use Ollama, you can modify `src/services/rag_pipeline.py` to use:

- OpenAI API
- Anthropic Claude
- Google Gemini
- Any other LLM service

## Troubleshooting

### Ollama Not Starting?

```bash
# Kill any existing Ollama processes
pkill ollama

# Start fresh
ollama serve
```

### Port Issues?

```bash
# Check if port 11434 is in use
lsof -i :11434

# Ollama default port is 11434
```

### Connection Issues from Docker?

The API container needs to reach Ollama on your host machine:

**Update `docker-compose.yml`:**

```yaml
api:
  environment:
    - OLLAMA_HOST=host.docker.internal # For macOS/Windows
    # or
    - OLLAMA_HOST=172.17.0.1 # For Linux
```

Then restart:

```bash
docker-compose down
docker-compose up -d
```

## Quick Start Guide

```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start your RAG system
docker-compose up

# Terminal 3: Test query
curl -X POST "http://localhost:8000/api/query?query=What%20is%20machine%20learning&top_k=5"
```

## Status Check

Run this to verify everything:

```bash
#!/bin/bash
echo "Checking system status..."
echo ""

# Check Docker services
echo "1. Docker Services:"
docker-compose ps
echo ""

# Check Ollama
echo "2. Ollama Status:"
if curl -s http://localhost:11434/api/version > /dev/null; then
    echo "‚úÖ Ollama is running"
else
    echo "‚ùå Ollama is not running - Start with: ollama serve"
fi
echo ""

# Check API
echo "3. API Status:"
if curl -s http://localhost:8000/docs > /dev/null; then
    echo "‚úÖ API is accessible"
else
    echo "‚ùå API is not accessible"
fi
echo ""
```

Save as `check_status.sh` and run:

```bash
chmod +x check_status.sh
./check_status.sh
```

## Summary

Your RAG system is **working perfectly**! You just need to:

1. **Install Ollama**: `curl https://ollama.ai/install.sh | sh`
2. **Pull model**: `ollama pull llama3`
3. **Start Ollama**: `ollama serve`
4. **Test again**: Query should now return LLM-generated answers!

That's it! üöÄ

---

**Need help?** Check the logs:

```bash
# API logs
docker-compose logs -f api

# Ollama logs (if using Docker)
docker logs -f ollama
```
